{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJI7qd_zYIMh"
   },
   "source": [
    "# Esta Notebook sirve para probar la generacion de respuestas de nuestros modelos entre usuarios de JPC y FDT\n",
    "\n",
    "Ejecutar en Google Colab o en si se quiere ejecutar de manera local se requiere Python 3.6 y tensorflow 1.15\n",
    "\n",
    "Se puede conseguir corriendo:\n",
    "\n",
    "    conda create --name GPT2GPU python=3.6 jupyter -y\n",
    "    conda activate GPT2GPU\n",
    "    conda install --name GPT2GPU tensorflow-gpu=1.15 --channel conda-forge -y\n",
    "\n",
    "ó para correrlo con CPU\n",
    "\n",
    "    conda create --name GPT2CPU python=3.6 jupyter -y\n",
    "    conda activate GPT2CPU\n",
    "    conda install --name GPT2CPU tensorflow=1.15 --channel conda-forge -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOYwDJhSYIJy"
   },
   "source": [
    "## Configuracion antes de ejecutar\n",
    "\n",
    "se pueden cambiar algunos valores aqui y volver a correr toda la notebook para que entrene y guarde otro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2M9348CYccL"
   },
   "outputs": [],
   "source": [
    "# Nombre del Modelo que vas a generar despues del finetunning, Ejemplo: FDT ó JPC\n",
    "MODEL_NAME = \"FDT\"   \n",
    "\n",
    "# COLAB or LOCAL\n",
    "ENVIRONMENT = \"LOCAL\" \n",
    "\n",
    "# TEXTO DE ENTRADA\n",
    "INPUT_TEXT= \"Que se vayan todos!\"\n",
    "\n",
    "\n",
    "### NO TOCAR\n",
    "MODEL_DIRS='models'\n",
    "if ENVIRONMENT==\"LOCAL\":\n",
    "    MODEL_DIRS ='../' + MODEL_DIRS\n",
    "    CHECKPOINT_DIR=MODEL_DIRS\n",
    "else:\n",
    "    CHECKPOINT_DIR=\"checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7LoMj4GA4n_"
   },
   "source": [
    "#  Empieza la notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si esto no dice que tenes la version 1.15 de tensorflow no va a funcionar!\n",
    "#!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "KBkpRgBCBS2_",
    "outputId": "d3a1ca20-b929-4c1b-a9e1-d5d67a45440a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if ENVIRONMENT==\"COLAB\":\n",
    "    %tensorflow_version 1.x\n",
    "    from google.colab import files\n",
    "else:\n",
    "    !pip install -q tensorflow-plot==0.3.0\n",
    "!pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bj2IJLHP3KwE"
   },
   "source": [
    "## GPU\n",
    "\n",
    "Colaboratory usa tanto una Nvidia T4 GPU o una Nvidia K80 GPU. La T4 es ligeramente mas rapida que la K80\n",
    "\n",
    "Podes verificar en que estas corriendo usando esta celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "sUmTooTW3osf",
    "outputId": "bba959a0-85ea-4862-b416-7a08c46f5e9b"
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8KXuKWzQSsN"
   },
   "source": [
    "## Montamos Google Drive\n",
    "\n",
    "En caso de usar modelos directo de Google Drive, dejarlos en la raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "puq4iC6vUAHc",
    "outputId": "d4d99a03-01a2-4950-f31f-1e9d36b5e4c2"
   },
   "outputs": [],
   "source": [
    "if ENVIRONMENT==\"COLAB\":\n",
    "    gpt2.mount_gdrive()\n",
    "    gpt2.copy_checkpoint_from_gdrive(run_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTa6zf3e_9gV"
   },
   "source": [
    "## Cargamos el modelo en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-fxL77nvAMAX",
    "outputId": "f6eb9ddb-b412-44c3-d6a6-e54dd1a30d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint ../models/FDT/model-2000\n",
      "INFO:tensorflow:Restoring parameters from ../models/FDT/model-2000\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name=MODEL_NAME,checkpoint_dir=MODEL_DIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClJwpF_ACONp"
   },
   "source": [
    "## Generamos el resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oF4-PqF0Fl7R"
   },
   "source": [
    "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
    "\n",
    "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
    "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "8DKMc0fiej4N",
    "outputId": "e541768c-54a4-4693-87f6-113ebdd21384"
   },
   "outputs": [],
   "source": [
    "responses = gpt2.generate(sess,\n",
    "              checkpoint_dir=CHECKPOINT_DIR,\n",
    "              run_name=MODEL_NAME,\n",
    "              length=250,\n",
    "              temperature=.9,\n",
    "              prefix=\"[POST] \" + INPUT_TEXT + \" \\n[RESPONSE]\",\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              truncate=\"<|endoftext|>\",\n",
    "              return_as_list=True,\n",
    "              include_prefix=False\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vemos El resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' @uberYbarra En lugar deuda y mierda yo, creo que llegó a una persona que miente !!! Que es una operación ! LOS COSAS !!!\\n',\n",
       " ' @_Trompeta Sí me acordarlo por los colores de lo que decías, se sostiene un nombre de puertas y por los trabajadores de @C5N. Acordarlo se vayan las exercisemen. Igual lo quiero.\\n',\n",
       " ' @_LauraDiMarco @Agos102005 Nadie es la sincera, que Alverso se va a poner la imagen desde que ML difundir la cenación.\\n',\n",
       " ' @Winston_Dunhill #YoHoyMiroABaby ¿y vos? ¿y vos dijo la verdad?\\n',\n",
       " ' @FernandezAnibal Era hacercelo posible para esto... https://t.co/7htZ26NpHD\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FineTunningDeGPT2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:GPT2CPU]",
   "language": "python",
   "name": "conda-env-GPT2CPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
